{
  "id": "4ce63510-bf23-4b7d-a00e-1b9b40428cdf",
  "instruction": "How will you evaluate a text summarizer system?",
  "input": "",
  "output": "The evaluation of a text summarizer system is typically performed by comparing its generated summaries against reference summaries. Such a comparison is typically done using metrics such as ROUGE, which measures the extent to which the generated summary overlaps with a reference summary. Other metrics such as semantic similarity metrics and human evaluation can also be used to evaluate a text summarizer system.",
  "license": "stanford-alpaca-unknown",
  "generation-type": "openai-text-davinci-003",
  "generation-reference": "https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json",
  "modified": [],
  "verified": [],
  "tags": []
}