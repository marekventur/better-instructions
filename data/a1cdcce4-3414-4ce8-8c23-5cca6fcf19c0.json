{
  "id": "a1cdcce4-3414-4ce8-8c23-5cca6fcf19c0",
  "instruction": "Name an example of a situation where an AI system made a biased decision",
  "input": "",
  "output": "In 2016, Amazon's facial recognition software incorrectly matched 28 members of Congress with imaging from a public database of criminal mugshots. This example highlighted how AI systems can make biased decisions based on racial and gender bias which is present in the data it is trained on.",
  "license": "stanford-alpaca-unknown",
  "generation-type": "openai-text-davinci-003",
  "generation-reference": "https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json",
  "modified": [],
  "verified": [],
  "tags": []
}